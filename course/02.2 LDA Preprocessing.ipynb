{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset(path):\n",
    "\twith open(path) as f:\n",
    "\t\tdata = json.load(f)\n",
    "\t\n",
    "\ttexts = [ d['text'] for d in data ]\n",
    "\tlabels = [ d['label'] for d in data ]\n",
    "\treturn texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/articles/train.json\"\n",
    "texts, labels = load_dataset(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative nature of LDA does not account for context around words, but rather operates on a bag-of-words view of each document.\n",
    "This means, the order of words in a document is not considered, but rather only the frequency of words in a document.\n",
    "In particular, two words need to be spelled the same way to be considered the same word. As many languages, including English, have different forms of the same word, we need to normalize the words in our documents. This is called lemmatization.\n",
    "\n",
    "Further, we want to remove as much noise as possible from our documents. This includes removing punctuation, numbers, and common words that do not carry much meaning, such as \"the\", \"and\", \"a\", etc. These words are called stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize(text):\n",
    "\tdoc = lemmatizer(text) \n",
    "\treturn [ token.lemma_ for token in doc if not token.is_punct and not token.is_space ]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\treturn [ token for token in text if not token in STOP_WORDS ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized version of the example is:\n",
      "the quick brown fox jump over the lazy dog this then be another sentence i.e. another heap of word\n",
      "\n",
      "Removing the stopwords yields\n",
      "quick brown fox jump lazy dog sentence i.e. heap word\n"
     ]
    }
   ],
   "source": [
    "example = \"The quick brown fox jumps over the lazy dogs. This then is another sentence, i.e., another heap of words.\"\n",
    "example_lemmatized = lemmatize(example)\n",
    "\n",
    "print(\"The lemmatized version of the example is:\")\n",
    "print(\" \".join(example_lemmatized))\n",
    "\n",
    "print(\"\\nRemoving the stopwords yields\")\n",
    "print(\" \".join(remove_stopwords(example_lemmatized)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization step also lowercased all of our words, which is a common practice.\n",
    "\n",
    "Let's apply this pipeline to our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    return [ remove_stopwords(lemmatize(text)) for text in tqdm(texts, ncols=80) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 24000/24000 [01:47<00:00, 222.92it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(path):\n",
    "\ttexts, _ = load_dataset(path)\n",
    "\tresult = preprocess_corpus(texts)\n",
    " \n",
    "\twith open(path.replace(\".json\", \".preprocessed.json\"), \"w\") as f:\n",
    "\t\tjson.dump(result, f)\n",
    "  \n",
    "#preprocess(\"../data/articles/train.json\")\n",
    "#preprocess(\"../data/articles/test.json\")\n",
    "\n",
    "#preprocess(\"../data/headlines/train.json\")\n",
    "preprocess(\"../data/headlines/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
