{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic and Other Embeddings-Based Topic Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA, like most classical topic models, understands words as distinct features of documents, where the appearances of words in documents are independent of each other, except for the topic-word distribution which the model learns.\n",
    "This assumption is not true in real life, where words are semantically related to each other. For example, the words \"dog\" and \"cat\" are more similar than \"dog\" and \"apple\".\n",
    "\n",
    "In this way, Embeddings-Based Topic Models (EBTM) can be understood as introducing a prior on the topic-word distribution, which is based on the pretrained semantic similarity between words. This prior is expressed as an additional layer of abstraction, where the topic-word distribution is a function of the word embeddings, instead of the word counts themselves.\n",
    "\n",
    "In contrast to LDA, most EBTM models, hence, are not generative. Instead, they follow a bottom-up approach. First, documents are embedded in the new, semantic embedding space, and then, topics are extracted from the documents in this new space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/graphics/bertopic-overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every cluster constitutes a topic. Some clustering algorithms (including HDBSCAN, which is commonly used), also generate a hierarchical cluster structure, where small topics are combined into bigger ones, allowing the user to easily investigate their corpus at different granularities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike LDA, there is no obvious, \"built-in\" way to generate descriptions for each topic, as there is no generative model in the background. A common way to construct meaningful summaries is c-TF-IDF, which combines all documents of a topic into one and then computes TF-IDF vectors on those documents.\n",
    "Each topic can subsequently be characterized by those words, that yield the highest TF-IDF scores.\n",
    "\n",
    "Notice how this method is not limited to Emebedding-Based TMs but rather can be applied to any corpus of topic-annotated documents, including LDA classifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large part of this part of the course is very close to the excellent BERTopic [documentation](https://maartengr.github.io/BERTopic/)](https://maartengr.github.io/BERTopic/), which I recommend you read.\n",
    "\n",
    "BERTopic is a plug-and-play topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
    "It works by composing five submodules, which can be subsumed in two overarching steps:\n",
    "\n",
    "**Topic Generation**\n",
    "\n",
    "This part of the pipeline takes the raw document as input and outputs topic labels for each document.\n",
    "\n",
    "* *Document Embedding* is the process of encoding documents into vectors. By default, BERTopic uses the [sentence-transformers](https://www.sbert.net/) library to embed documents in a semantic embedding space.\n",
    "* *Dimensionality Reduction* is used to make the downstream tasks more tractable. In addition, some methods allow you to infuse the reduction with annotations, such as the default, [UMAP](https://umap-learn.readthedocs.io/en/latest/), in the case of semi-supervised topic modeling.\n",
    "* *Clustering* consumes the dimensionality-reduced embeddings and groups similar documents together in topics, performing the actual generation step. By default, BERTopic uses [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) for clustering.\n",
    "\n",
    "**Topic Inspection**\n",
    "\n",
    "Topic Inspection, using the raw documents and labels from the previous section, aims to generate intelligible descriptions/labels for each topic, which can be used for interpretation.\n",
    "\n",
    "* *Vectorization* chunks the raw documents into tokens, e.g. words/n-grams of words, and counts their occurrences. By default, BERTopic uses the Sklearn implementation of [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for this step.\n",
    "* *Description Creation* generates a description for each topic, given its tokens and token counts. By default, BERTopic uses the [c-TF-IDF](https://www.aclweb.org/anthology/W04-3252.pdf) algorithm to generate the descriptions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Advantages of BERTopic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTopic has two key advantages in its architecture. Firstly, it makes very sensible choices for its default parameters, making it one of the easiest, yet most powerful topic modeling techniques to use.\n",
    "Even if you have no prior knowledge of topic modeling, \n",
    "```\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "```\n",
    "is very likely to give you results en-par with a tuned LDA model.\n",
    "\n",
    "Secondly, it is very flexible, allowing you to easily change the submodules to your liking.\n",
    "This may either be to accommodate the kind of data you are working with, e.g. by using a different document embedding model, or to inject your domain knowledge into each part of the pipeline, e.g. by using a different/guided clustering algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other LLM-Based Topic Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BERTopic is a very powerful tool, it might not be able to accommodate every single use case.\n",
    "However, the general nature of its architecture will probably constitute a good starting point for most use cases, even if you need to build your pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please continue with the next [notebook](BERTopic-02-Preprocessing.ipynb)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
