{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike LDA, BERTopic requires barely any preprocessing. In particular, neither lemmatization nor stopword removal is required or even recommended. LLMs are trained to understand natural language and therefore, they can handle typos, slang, and stopwords.\n",
    "\n",
    "However, herein lies the problem: LLMs are trained on their dataset and not on the language of your domain. Therefore, the LLM might not have seen language specific to your domain before. In that case, it is recommended to use a domain-specific language model. For example, if you are working in the medical domain, you can use BioBERT or SciBERT. If you are working in the legal domain, you can use the LegalBERT model.\n",
    "\n",
    "BERTopic now supports Huggingface Models, which makes model selection very easy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
