{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 LDA Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative nature of LDA does not account for context around words, but rather operates on a bag-of-words view of each document.\n",
    "This means, the order of words in a document is not considered, but rather only the frequency of words in a document.\n",
    "In particular, two words need to be spelled the same way to be considered the same word. As many languages, including English, have different forms of the same word, we need to normalize the words in our documents. This is called lemmatization.\n",
    "\n",
    "Further, we want to remove as much noise as possible from our documents. This includes removing punctuation, numbers, and common words that do not carry much meaning, such as \"the\", \"and\", \"a\", etc. These words are called stop words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = [d['text'] for d in data]\n",
    "    labels = [d['label'] for d in data]\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "path = \"../data/articles/train.json\"\n",
    "texts, labels = load_dataset(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our simple preprocessing-pipeline we use spaCy for both lemmatization and stopword-removal. Other popular options include NLTK and gensim.\n",
    "\n",
    "It is sensible to perform lemmatization first, as this will reduce the number of word-forms that need to be checked for stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# We only need the lemmatizer, so we disable the parser and ner\n",
    "# en_core_web_md is a medium-sized model trained on written web text\n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "# Larger models, such as en_core_web_lg, are more accurate but take longer to load\n",
    "lemmatizer = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "\n",
    "# We extend the list of stopwords with our own words.\n",
    "# This is usually an iterative process, where you try out the model and add/remove words\n",
    "stopwords = STOP_WORDS.union(set([\"Mr.\", \"Mrs.\", \"Ms.\", \"Dr.\", \"$\", \"s\"]))\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = lemmatizer(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [token for token in text if not token in stopwords]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    return remove_stopwords(lemmatize(text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized version of the example is:\n",
      ">> Mr. Brown Fox jump this then be another sentence i.e. another heap of word thank you Weizenbaum Institute\n",
      "\n",
      "Removing the stopwords yields\n",
      ">> Brown Fox jump sentence i.e. heap word thank Weizenbaum Institute\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"Mr. Brown Fox jumps. \n",
    "This then is another sentence, i.e., another heap of words. \n",
    "Thank you, Weizenbaum Institute.\"\"\"\n",
    "\n",
    "example_lemmatized = lemmatize(example)\n",
    "\n",
    "print(\"The lemmatized version of the example is:\", end=\"\\n>> \")\n",
    "print(\" \".join(example_lemmatized))\n",
    "\n",
    "print(\"\\nRemoving the stopwords yields\", end=\"\\n>> \")\n",
    "print(\" \".join(remove_stopwords(example_lemmatized)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline lemmatized all words, removed punctuation, and stop words.\n",
    "The lemmatization step also lowercased all of our words except for proper nouns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the preprocessing pipeline to all documents in our dataset. As this may take a while, we will skip a document if it has already been preprocessed. If you want to re-run the preprocessing, you can delete the corresponding `.preprocessed.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2977/2977 [02:45<00:00, 18.00it/s]\n",
      "100%|█████████████████████████████████████████| 745/745 [00:38<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/headlines/train.preprocessed.json already exists, not overwriting.\n",
      "File ../data/headlines/test.preprocessed.json already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def preprocess_file(path):\n",
    "    output_filename = path.replace(\".json\", \".preprocessed.json\")\n",
    "    if exists(output_filename):\n",
    "        print(\"File {} already exists, not overwriting.\".format(output_filename))\n",
    "        return\n",
    "\n",
    "    texts, _ = load_dataset(path)\n",
    "    result = [preprocess(text) for text in tqdm(texts, ncols=80)]\n",
    "\n",
    "    with open(path.replace(\".json\", \".preprocessed.json\"), \"w\") as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "\n",
    "preprocess_file(\"../data/articles/train.json\")\n",
    "preprocess_file(\"../data/articles/test.json\")\n",
    "\n",
    "preprocess_file(\"../data/headlines/train.json\")\n",
    "preprocess_file(\"../data/headlines/test.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing is done!**\n",
    "\n",
    "*Please contiune with [the next notebook](LDA-03-Training.ipynb).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
