{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Topic Modeling?\n",
    "\n",
    "**Topic Modeling is family of techniques that aim to discover latent topical structures in text corpora.**\n",
    "Most commonly, topic modeling techniques are unsupervised, which means that they are applied to a text corpus without any prior knowledge about its inherent topics. \n",
    "\n",
    "The output of topic models is twofold: Firstly, they associate each document in the corpus with a topic, or a probability distribution over all topics. Secondly, they generate a comprehensive representation for each topic. This can be a list of common words, a list of representative documents, or a list of both.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use Topic Modeling\n",
    "\n",
    "Topic Modeling is commonly used for exploratory analysis. It can help to get a first impression of a large corpus of text documents, that is too large for manual inspection. Topic models can also be used to improve the performance of other text-mining tasks, such as document classification or clustering.\n",
    "\n",
    "Topic-Modeling can also prove useful for longitudinal analysis of text corpora. By applying topic models to different versions of a corpus, one can track the evolution of topics over time, as well as changes in the topic distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When not to use Topic Modeling\n",
    "\n",
    "Topic Modeling is not a good choice for text corpora that are too small. To discover meaningful topics, a corpus should roughly contain at least 1000 documents.\n",
    "\n",
    "Short texts might also pose a problem for topic models. If each document contains only a few words, and the words between documents have little overlap, it can be hard or even impossible to discover meaningful topics. This can partially be mitigated by LLM-based topic models, as we will discuss [later](BERTopic-01-Theory.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Models need to follow a general architecture consisting of preprocessing, training, inference, and description."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/graphics/pipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing** prepares the text corpus for use in the topic model. For classical models, this typically includes tokenization, lemmaization, and stopword removal. \n",
    "For more modern, embedding-based models, preprocessing is often limited to removing unwanted characters, such as punctuation or scraping artifacts, like HTML tags.\n",
    "\n",
    "In the **training** step, the model ingests the preprocessed data and learns the latent topic structure of the corpus. This step is computationally the most expensive and can take several hours or even days for large corpora.\n",
    "\n",
    "**Inference** uses the trained model to assign a topic label to each document in the corpus. This is usually a fast operation and can be done in a matter of seconds. Inference can also be applied to new documents, that were not part of the training corpus. Some models can also classify documents as _out-of-domain_ if they do not fit the learned topic structure.\n",
    "\n",
    "The trained model contains an abstract representation of each topic, which can be as little as a single inferred topic label per document. In the **description** step, the pipeline generates a human-interpretable representation of each topic, such as a list of words or representative documents.\n",
    "\n",
    "While the actual *topic model* only refers to the model that is trained in the second step, the first and last steps are closely tied to the model, and are often implemented as part of the same software package.\n",
    "Nonetheless, it is important to keep in mind that the preprocessing and description are separate steps, and their methods can (not always sensibly) be interchanged between different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Model Families"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though topic models can be characterized along a broad variety of axis, such as the amount of prior knowledge they can operate on, from a technical perspective it makes sense to distinguish between generative and discriminative models.\n",
    "\n",
    "### Generative Models\n",
    "Generative models use a probabilistic process to generate documents from topics. During training, the model tunes its parameters in such a way, that the given corpus becomes its most likely output.\n",
    "The inference is performed by checking which topic is most likely to have generated a given document. This is usually done by computing the probability of each topic generating the document and then selecting the topic with the highest probability.\n",
    "\n",
    "### Discriminative Models \n",
    "Discriminative models can not generate documents from topics. Instead, they directly learn a function that maps documents to topics. These models typically consist of a (pre-trained) embedding step, followed by clustering or classification.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup of this Course"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course focuses on two main models. **LDA** (Latent Dirichlet Allocation), a generative model, is probably the most widely used topic model. **BERTopic** (Bert-Embeddings for Topic Modeling), a discriminative model, is a very recent project that aims to provide a general network for topic modeling with BERT-embeddings.\n",
    "\n",
    "For both models, we provide four notebooks. The first notebook introduces the model and explains the theory behind it. The second notebook elaborates on the preprocessing that is required and recommended to achieve the best results. In the third notebook, we show how to use the model for inference, and how to interpret the results. The fourth notebook provides hints to advanced methods and special use cases, for which the model at hand might be useful.\n",
    "\n",
    "The entire course is meant to be plug-and-play. We provide it in the form of notebooks to provide maximal convenience for the reader to experiment with the models.\n",
    "While we provide two sample datasets, we encourage the reader to apply the models to their own data, and to experiment with the parameters and preprocessing steps.\n",
    "\n",
    "For setup instructions, please refer to the [README](../README.md) file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by [downloading the datasets, and afterward proceed to the first notebook of the model of your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
